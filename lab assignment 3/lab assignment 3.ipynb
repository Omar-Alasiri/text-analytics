{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c44c5d9-e9b3-4452-a613-c01fbeb955fb",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8502d9-3a04-4e6c-90f0-af08eb92eb93",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 981.5/981.5 kB 6.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\thema\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993251 sha256=76dd07d05aebca162dd9961dad0306c361dd93ad306ab62562d9d9c51c2d19cd\n",
      "  Stored in directory: c:\\users\\thema\\appdata\\local\\pip\\cache\\wheels\\c1\\67\\88\\e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e674209d-39a9-4af5-81f7-ca0a8f965cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\thema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import ast\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import re\n",
    "\n",
    "# Download NLTK content\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2fd453-7d1b-4084-a6cf-8f3250528282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>I recently posted an article asking what kind ...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>\\nIt depends on your priorities.  A lot of peo...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>: Ford and his automobile.  I need information...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  target  \\\n",
       "0           0  I was wondering if anyone out there could enli...       7   \n",
       "1          17  I recently posted an article asking what kind ...       7   \n",
       "2          29  \\nIt depends on your priorities.  A lot of peo...       7   \n",
       "3          56  an excellent automatic can be found in the sub...       7   \n",
       "4          64  : Ford and his automobile.  I need information...       7   \n",
       "\n",
       "       title                        date  \n",
       "0  rec.autos  2022-08-02 13:48:37.251043  \n",
       "1  rec.autos  2022-08-02 13:48:37.251043  \n",
       "2  rec.autos  2022-08-02 13:48:37.251043  \n",
       "3  rec.autos  2022-08-02 13:48:37.251043  \n",
       "4  rec.autos  2022-08-02 13:48:37.251043  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the dataset\n",
    "df = pd.read_csv(\"news_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f117d8-2652-4764-bd1a-cbac2095889b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        I was wondering if anyone out there could enli...\n",
       "1        I recently posted an article asking what kind ...\n",
       "2        \\nIt depends on your priorities.  A lot of peo...\n",
       "3        an excellent automatic can be found in the sub...\n",
       "4        : Ford and his automobile.  I need information...\n",
       "                               ...                        \n",
       "11309    Secrecy in Clipper Chip\\n\\nThe serial number o...\n",
       "11310    Hi !\\n\\nI am interested in the source of FEAL ...\n",
       "11311    The actual algorithm is classified, however, t...\n",
       "11312    \\n\\tThis appears to be generic calling upon th...\n",
       "11313    \\nProbably keep quiet and take it, lest they g...\n",
       "Name: text, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b83f2af8-e3bf-45bd-9627-6d25f681ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and null values\n",
    "df_cleaned = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13ef707d-d870-4332-b9b2-0519ecc9b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert the text to lower case\n",
    "df[\"lowercased\"] = df[\"text\"].astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "198bd919-489a-4fe4-a502-0f45d5a0b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Remove slangs from the text\n",
    "slang_dict = {\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"ikr\": \"I know right\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"bff\": \"best friend forever\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"hmu\": \"hit me up\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"w\": \"win\",\n",
    "    \"im\": \"i'm\"\n",
    "}\n",
    "\n",
    "def replace_slang(text):\n",
    "    escaped_slang_words = []\n",
    "    for word in slang_dict.keys():\n",
    "        escaped_word = re.escape(word)\n",
    "        escaped_slang_words.append(escaped_word)\n",
    "    slang_pattern = r'\\b(' + '|'.join(escaped_slang_words) + r')\\b'\n",
    "    \n",
    "    def replace_match(match):\n",
    "        slang_word = match.group(0)\n",
    "        return slang_dict[slang_word.lower()]\n",
    "    replaced_text = re.sub(slang_pattern, replace_match, text, flags=re.IGNORECASE)\n",
    "    return replaced_text\n",
    "\n",
    "df['slangs_replaced'] = df['lowercased'].apply(replace_slang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aed104b9-ba2d-4910-b968-c1bd9387d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Replace contrations in the text with full clause\n",
    "contractions_dict = {\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"why's\": \"why is\"\n",
    "}\n",
    "\n",
    "escaped_contractions = []\n",
    "\n",
    "for contraction in contractions_dict.keys():\n",
    "    escaped_contraction = re.escape(contraction)\n",
    "    escaped_contractions.append(escaped_contraction)\n",
    "\n",
    "joined_contractions = \"|\".join(escaped_contractions)\n",
    "contractions_pattern = r'\\b(' + joined_contractions + r')\\b'\n",
    "compiled_pattern = re.compile(contractions_pattern, flags=re.IGNORECASE)\n",
    "\n",
    "def replaced_contractions(text):\n",
    "    def replace_match(match):\n",
    "        matched_word = match.group(0)  # Extract matched contraction\n",
    "        lower_matched_word = matched_word.lower()  # Convert to lowercase\n",
    "        expanded_form = contractions_dict[lower_matched_word]  # Get full form from dictionary\n",
    "        return expanded_form  # Return the expanded form\n",
    "\n",
    "    expanded_text = compiled_pattern.sub(replace_match, text)\n",
    "\n",
    "    return expanded_text\n",
    "\n",
    "df['contractions_replaced'] = df['slangs_replaced'].apply(replaced_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "251d81e2-faff-4df8-9aae-918e3f23e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Remove punctuation from text\n",
    "import string\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "df[\"punctuations_removed\"] = df[\"contractions_replaced\"].apply(remove_punctuation)\n",
    "\n",
    "# Step 5: Remove numbers form the text\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "df['numbers_removed'] = df['punctuations_removed'].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08fc779b-1dd8-45ec-89a5-168acd224879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Remove emojis from the text\n",
    "import emoji\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "df['emoji_removed'] = df['punctuations_removed'].apply(remove_emojis)\n",
    "\n",
    "# Step 7: Remove stopwords from the text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        lower_word = word.lower()\n",
    "        if lower_word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df[\"stopwords_removed\"] = df[\"emoji_removed\"].apply(remove_stopwords)\n",
    "\n",
    "# Step 8: Stem all the text\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "df[\"stemmed_words\"] = df[\"stopwords_removed\"].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51b78a80-3622-413f-b507-5604c058f21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\thema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\thema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\thema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\thema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Lemmatize the text\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "df[\"lemmatized\"] = df[\"stopwords_removed\"].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abece7d5-8663-41c4-a34b-cf08d113fecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Tokenize all the text\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df[\"tokenized\"] = df[\"lemmatized\"].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b547b2b-4efe-4e1f-8059-42305e2dff91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>lowercased</th>\n",
       "      <th>slangs_replaced</th>\n",
       "      <th>contractions_replaced</th>\n",
       "      <th>punctuations_removed</th>\n",
       "      <th>numbers_removed</th>\n",
       "      <th>emoji_removed</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>stemmed_words</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>i was wondering if anyone out there could enli...</td>\n",
       "      <td>i was wondering if anyone out there could enli...</td>\n",
       "      <td>i was wondering if anyone out there could enli...</td>\n",
       "      <td>i was wondering if anyone out there could enli...</td>\n",
       "      <td>i was wondering if anyone out there could enli...</td>\n",
       "      <td>i was wondering if anyone out there could enli...</td>\n",
       "      <td>wondering anyone could enlighten car saw day 2...</td>\n",
       "      <td>wonder anyon could enlighten car saw day 2door...</td>\n",
       "      <td>wonder anyone could enlighten car saw day 2doo...</td>\n",
       "      <td>[wonder, anyone, could, enlighten, car, saw, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>I recently posted an article asking what kind ...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>i recently posted an article asking what kind ...</td>\n",
       "      <td>i recently posted an article asking what kind ...</td>\n",
       "      <td>i recently posted an article asking what kind ...</td>\n",
       "      <td>i recently posted an article asking what kind ...</td>\n",
       "      <td>i recently posted an article asking what kind ...</td>\n",
       "      <td>i recently posted an article asking what kind ...</td>\n",
       "      <td>recently posted article asking kind rates sing...</td>\n",
       "      <td>recent post articl ask kind rate singl male dr...</td>\n",
       "      <td>recently post article ask kind rate single mal...</td>\n",
       "      <td>[recently, post, article, ask, kind, rate, sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>\\nIt depends on your priorities.  A lot of peo...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>\\nit depends on your priorities.  a lot of peo...</td>\n",
       "      <td>\\nit depends on your priorities.  a lot of peo...</td>\n",
       "      <td>\\nit depends on your priorities.  a lot of peo...</td>\n",
       "      <td>\\nit depends on your priorities  a lot of peop...</td>\n",
       "      <td>\\nit depends on your priorities  a lot of peop...</td>\n",
       "      <td>\\nit depends on your priorities  a lot of peop...</td>\n",
       "      <td>depends priorities lot people put higher prior...</td>\n",
       "      <td>depend prioriti lot peopl put higher prioriti ...</td>\n",
       "      <td>depend priority lot people put high priority g...</td>\n",
       "      <td>[depend, priority, lot, people, put, high, pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>excellent automatic found subaru legacy switch...</td>\n",
       "      <td>excel automat found subaru legaci switch sport...</td>\n",
       "      <td>excellent automatic find subaru legacy switch ...</td>\n",
       "      <td>[excellent, automatic, find, subaru, legacy, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>: Ford and his automobile.  I need information...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>: ford and his automobile.  i need information...</td>\n",
       "      <td>: ford and his automobile.  i need information...</td>\n",
       "      <td>: ford and his automobile.  i need information...</td>\n",
       "      <td>ford and his automobile  i need information o...</td>\n",
       "      <td>ford and his automobile  i need information o...</td>\n",
       "      <td>ford and his automobile  i need information o...</td>\n",
       "      <td>ford automobile need information whether ford ...</td>\n",
       "      <td>ford automobil need inform whether ford partia...</td>\n",
       "      <td>ford automobile need information whether ford ...</td>\n",
       "      <td>[ford, automobile, need, information, whether,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>11210</td>\n",
       "      <td>Secrecy in Clipper Chip\\n\\nThe serial number o...</td>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>secrecy in clipper chip\\n\\nthe serial number o...</td>\n",
       "      <td>secrecy in clipper chip\\n\\nthe serial number o...</td>\n",
       "      <td>secrecy in clipper chip\\n\\nthe serial number o...</td>\n",
       "      <td>secrecy in clipper chip\\n\\nthe serial number o...</td>\n",
       "      <td>secrecy in clipper chip\\n\\nthe serial number o...</td>\n",
       "      <td>secrecy in clipper chip\\n\\nthe serial number o...</td>\n",
       "      <td>secrecy clipper chip serial number clipper chi...</td>\n",
       "      <td>secreci clipper chip serial number clipper chi...</td>\n",
       "      <td>secrecy clipper chip serial number clipper chi...</td>\n",
       "      <td>[secrecy, clipper, chip, serial, number, clipp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>11217</td>\n",
       "      <td>Hi !\\n\\nI am interested in the source of FEAL ...</td>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>hi !\\n\\ni am interested in the source of feal ...</td>\n",
       "      <td>hi !\\n\\ni am interested in the source of feal ...</td>\n",
       "      <td>hi !\\n\\ni am interested in the source of feal ...</td>\n",
       "      <td>hi \\n\\ni am interested in the source of feal e...</td>\n",
       "      <td>hi \\n\\ni am interested in the source of feal e...</td>\n",
       "      <td>hi \\n\\ni am interested in the source of feal e...</td>\n",
       "      <td>hi interested source feal encryption algorithm...</td>\n",
       "      <td>hi interest sourc feal encrypt algorithm someo...</td>\n",
       "      <td>hi interested source feal encryption algorithm...</td>\n",
       "      <td>[hi, interested, source, feal, encryption, alg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>11243</td>\n",
       "      <td>The actual algorithm is classified, however, t...</td>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>the actual algorithm is classified, however, t...</td>\n",
       "      <td>the actual algorithm is classified, however, t...</td>\n",
       "      <td>the actual algorithm is classified, however, t...</td>\n",
       "      <td>the actual algorithm is classified however the...</td>\n",
       "      <td>the actual algorithm is classified however the...</td>\n",
       "      <td>the actual algorithm is classified however the...</td>\n",
       "      <td>actual algorithm classified however main thrus...</td>\n",
       "      <td>actual algorithm classifi howev main thrust ce...</td>\n",
       "      <td>actual algorithm classify however main thrust ...</td>\n",
       "      <td>[actual, algorithm, classify, however, main, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>11254</td>\n",
       "      <td>\\n\\tThis appears to be generic calling upon th...</td>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>\\n\\tthis appears to be generic calling upon th...</td>\n",
       "      <td>\\n\\tthis appears to be generic calling upon th...</td>\n",
       "      <td>\\n\\tthis appears to be generic calling upon th...</td>\n",
       "      <td>\\n\\tthis appears to be generic calling upon th...</td>\n",
       "      <td>\\n\\tthis appears to be generic calling upon th...</td>\n",
       "      <td>\\n\\tthis appears to be generic calling upon th...</td>\n",
       "      <td>appears generic calling upon name antichrist h...</td>\n",
       "      <td>appear gener call upon name antichrist hell le...</td>\n",
       "      <td>appear generic call upon name antichrist hell ...</td>\n",
       "      <td>[appear, generic, call, upon, name, antichrist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>11302</td>\n",
       "      <td>\\nProbably keep quiet and take it, lest they g...</td>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>\\nprobably keep quiet and take it, lest they g...</td>\n",
       "      <td>\\nprobably keep quiet and take it, lest they g...</td>\n",
       "      <td>\\nprobably keep quiet and take it, lest they g...</td>\n",
       "      <td>\\nprobably keep quiet and take it lest they ge...</td>\n",
       "      <td>\\nprobably keep quiet and take it lest they ge...</td>\n",
       "      <td>\\nprobably keep quiet and take it lest they ge...</td>\n",
       "      <td>probably keep quiet take lest get kneecaps busted</td>\n",
       "      <td>probabl keep quiet take lest get kneecap bust</td>\n",
       "      <td>probably keep quiet take l get kneecap bust</td>\n",
       "      <td>[probably, keep, quiet, take, l, get, kneecap,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text  target  \\\n",
       "0               0  I was wondering if anyone out there could enli...       7   \n",
       "1              17  I recently posted an article asking what kind ...       7   \n",
       "2              29  \\nIt depends on your priorities.  A lot of peo...       7   \n",
       "3              56  an excellent automatic can be found in the sub...       7   \n",
       "4              64  : Ford and his automobile.  I need information...       7   \n",
       "...           ...                                                ...     ...   \n",
       "11309       11210  Secrecy in Clipper Chip\\n\\nThe serial number o...      11   \n",
       "11310       11217  Hi !\\n\\nI am interested in the source of FEAL ...      11   \n",
       "11311       11243  The actual algorithm is classified, however, t...      11   \n",
       "11312       11254  \\n\\tThis appears to be generic calling upon th...      11   \n",
       "11313       11302  \\nProbably keep quiet and take it, lest they g...      11   \n",
       "\n",
       "           title                        date  \\\n",
       "0      rec.autos  2022-08-02 13:48:37.251043   \n",
       "1      rec.autos  2022-08-02 13:48:37.251043   \n",
       "2      rec.autos  2022-08-02 13:48:37.251043   \n",
       "3      rec.autos  2022-08-02 13:48:37.251043   \n",
       "4      rec.autos  2022-08-02 13:48:37.251043   \n",
       "...          ...                         ...   \n",
       "11309  sci.crypt  2022-08-02 13:48:37.251043   \n",
       "11310  sci.crypt  2022-08-02 13:48:37.251043   \n",
       "11311  sci.crypt  2022-08-02 13:48:37.251043   \n",
       "11312  sci.crypt  2022-08-02 13:48:37.251043   \n",
       "11313  sci.crypt  2022-08-02 13:48:37.251043   \n",
       "\n",
       "                                              lowercased  \\\n",
       "0      i was wondering if anyone out there could enli...   \n",
       "1      i recently posted an article asking what kind ...   \n",
       "2      \\nit depends on your priorities.  a lot of peo...   \n",
       "3      an excellent automatic can be found in the sub...   \n",
       "4      : ford and his automobile.  i need information...   \n",
       "...                                                  ...   \n",
       "11309  secrecy in clipper chip\\n\\nthe serial number o...   \n",
       "11310  hi !\\n\\ni am interested in the source of feal ...   \n",
       "11311  the actual algorithm is classified, however, t...   \n",
       "11312  \\n\\tthis appears to be generic calling upon th...   \n",
       "11313  \\nprobably keep quiet and take it, lest they g...   \n",
       "\n",
       "                                         slangs_replaced  \\\n",
       "0      i was wondering if anyone out there could enli...   \n",
       "1      i recently posted an article asking what kind ...   \n",
       "2      \\nit depends on your priorities.  a lot of peo...   \n",
       "3      an excellent automatic can be found in the sub...   \n",
       "4      : ford and his automobile.  i need information...   \n",
       "...                                                  ...   \n",
       "11309  secrecy in clipper chip\\n\\nthe serial number o...   \n",
       "11310  hi !\\n\\ni am interested in the source of feal ...   \n",
       "11311  the actual algorithm is classified, however, t...   \n",
       "11312  \\n\\tthis appears to be generic calling upon th...   \n",
       "11313  \\nprobably keep quiet and take it, lest they g...   \n",
       "\n",
       "                                   contractions_replaced  \\\n",
       "0      i was wondering if anyone out there could enli...   \n",
       "1      i recently posted an article asking what kind ...   \n",
       "2      \\nit depends on your priorities.  a lot of peo...   \n",
       "3      an excellent automatic can be found in the sub...   \n",
       "4      : ford and his automobile.  i need information...   \n",
       "...                                                  ...   \n",
       "11309  secrecy in clipper chip\\n\\nthe serial number o...   \n",
       "11310  hi !\\n\\ni am interested in the source of feal ...   \n",
       "11311  the actual algorithm is classified, however, t...   \n",
       "11312  \\n\\tthis appears to be generic calling upon th...   \n",
       "11313  \\nprobably keep quiet and take it, lest they g...   \n",
       "\n",
       "                                    punctuations_removed  \\\n",
       "0      i was wondering if anyone out there could enli...   \n",
       "1      i recently posted an article asking what kind ...   \n",
       "2      \\nit depends on your priorities  a lot of peop...   \n",
       "3      an excellent automatic can be found in the sub...   \n",
       "4       ford and his automobile  i need information o...   \n",
       "...                                                  ...   \n",
       "11309  secrecy in clipper chip\\n\\nthe serial number o...   \n",
       "11310  hi \\n\\ni am interested in the source of feal e...   \n",
       "11311  the actual algorithm is classified however the...   \n",
       "11312  \\n\\tthis appears to be generic calling upon th...   \n",
       "11313  \\nprobably keep quiet and take it lest they ge...   \n",
       "\n",
       "                                         numbers_removed  \\\n",
       "0      i was wondering if anyone out there could enli...   \n",
       "1      i recently posted an article asking what kind ...   \n",
       "2      \\nit depends on your priorities  a lot of peop...   \n",
       "3      an excellent automatic can be found in the sub...   \n",
       "4       ford and his automobile  i need information o...   \n",
       "...                                                  ...   \n",
       "11309  secrecy in clipper chip\\n\\nthe serial number o...   \n",
       "11310  hi \\n\\ni am interested in the source of feal e...   \n",
       "11311  the actual algorithm is classified however the...   \n",
       "11312  \\n\\tthis appears to be generic calling upon th...   \n",
       "11313  \\nprobably keep quiet and take it lest they ge...   \n",
       "\n",
       "                                           emoji_removed  \\\n",
       "0      i was wondering if anyone out there could enli...   \n",
       "1      i recently posted an article asking what kind ...   \n",
       "2      \\nit depends on your priorities  a lot of peop...   \n",
       "3      an excellent automatic can be found in the sub...   \n",
       "4       ford and his automobile  i need information o...   \n",
       "...                                                  ...   \n",
       "11309  secrecy in clipper chip\\n\\nthe serial number o...   \n",
       "11310  hi \\n\\ni am interested in the source of feal e...   \n",
       "11311  the actual algorithm is classified however the...   \n",
       "11312  \\n\\tthis appears to be generic calling upon th...   \n",
       "11313  \\nprobably keep quiet and take it lest they ge...   \n",
       "\n",
       "                                       stopwords_removed  \\\n",
       "0      wondering anyone could enlighten car saw day 2...   \n",
       "1      recently posted article asking kind rates sing...   \n",
       "2      depends priorities lot people put higher prior...   \n",
       "3      excellent automatic found subaru legacy switch...   \n",
       "4      ford automobile need information whether ford ...   \n",
       "...                                                  ...   \n",
       "11309  secrecy clipper chip serial number clipper chi...   \n",
       "11310  hi interested source feal encryption algorithm...   \n",
       "11311  actual algorithm classified however main thrus...   \n",
       "11312  appears generic calling upon name antichrist h...   \n",
       "11313  probably keep quiet take lest get kneecaps busted   \n",
       "\n",
       "                                           stemmed_words  \\\n",
       "0      wonder anyon could enlighten car saw day 2door...   \n",
       "1      recent post articl ask kind rate singl male dr...   \n",
       "2      depend prioriti lot peopl put higher prioriti ...   \n",
       "3      excel automat found subaru legaci switch sport...   \n",
       "4      ford automobil need inform whether ford partia...   \n",
       "...                                                  ...   \n",
       "11309  secreci clipper chip serial number clipper chi...   \n",
       "11310  hi interest sourc feal encrypt algorithm someo...   \n",
       "11311  actual algorithm classifi howev main thrust ce...   \n",
       "11312  appear gener call upon name antichrist hell le...   \n",
       "11313      probabl keep quiet take lest get kneecap bust   \n",
       "\n",
       "                                              lemmatized  \\\n",
       "0      wonder anyone could enlighten car saw day 2doo...   \n",
       "1      recently post article ask kind rate single mal...   \n",
       "2      depend priority lot people put high priority g...   \n",
       "3      excellent automatic find subaru legacy switch ...   \n",
       "4      ford automobile need information whether ford ...   \n",
       "...                                                  ...   \n",
       "11309  secrecy clipper chip serial number clipper chi...   \n",
       "11310  hi interested source feal encryption algorithm...   \n",
       "11311  actual algorithm classify however main thrust ...   \n",
       "11312  appear generic call upon name antichrist hell ...   \n",
       "11313        probably keep quiet take l get kneecap bust   \n",
       "\n",
       "                                               tokenized  \n",
       "0      [wonder, anyone, could, enlighten, car, saw, d...  \n",
       "1      [recently, post, article, ask, kind, rate, sin...  \n",
       "2      [depend, priority, lot, people, put, high, pri...  \n",
       "3      [excellent, automatic, find, subaru, legacy, s...  \n",
       "4      [ford, automobile, need, information, whether,...  \n",
       "...                                                  ...  \n",
       "11309  [secrecy, clipper, chip, serial, number, clipp...  \n",
       "11310  [hi, interested, source, feal, encryption, alg...  \n",
       "11311  [actual, algorithm, classify, however, main, t...  \n",
       "11312  [appear, generic, call, upon, name, antichrist...  \n",
       "11313  [probably, keep, quiet, take, l, get, kneecap,...  \n",
       "\n",
       "[11314 rows x 15 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "976dde8d-2a7b-45ee-9e10-ec2d9849e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to the cleaned dataset to a CSV file\n",
    "df[[\"text\", \"tokenized\"]].to_csv(\"cleaned_news_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fab096-2bf0-4f8e-ac0a-fb102b2846f8",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44b2cafa-9b5a-4eff-8bce-1156ec48df4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>lowercased</th>\n",
       "      <th>slangs_replaced</th>\n",
       "      <th>contractions_replaced</th>\n",
       "      <th>punctuations_removed</th>\n",
       "      <th>numbers_removed</th>\n",
       "      <th>emoji_removed</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>stemmed_words</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>i was wondering if anyone out there could enli...</td>\n",
       "      <td>i was wondering if anyone out there could enli...</td>\n",
       "      <td>i was wondering if anyone out there could enli...</td>\n",
       "      <td>i was wondering if anyone out there could enli...</td>\n",
       "      <td>i was wondering if anyone out there could enli...</td>\n",
       "      <td>i was wondering if anyone out there could enli...</td>\n",
       "      <td>wondering anyone could enlighten car saw day 2...</td>\n",
       "      <td>wonder anyon could enlighten car saw day 2door...</td>\n",
       "      <td>wonder anyone could enlighten car saw day 2doo...</td>\n",
       "      <td>[wonder, anyone, could, enlighten, car, saw, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>I recently posted an article asking what kind ...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>i recently posted an article asking what kind ...</td>\n",
       "      <td>i recently posted an article asking what kind ...</td>\n",
       "      <td>i recently posted an article asking what kind ...</td>\n",
       "      <td>i recently posted an article asking what kind ...</td>\n",
       "      <td>i recently posted an article asking what kind ...</td>\n",
       "      <td>i recently posted an article asking what kind ...</td>\n",
       "      <td>recently posted article asking kind rates sing...</td>\n",
       "      <td>recent post articl ask kind rate singl male dr...</td>\n",
       "      <td>recently post article ask kind rate single mal...</td>\n",
       "      <td>[recently, post, article, ask, kind, rate, sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>\\nIt depends on your priorities.  A lot of peo...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>\\nit depends on your priorities.  a lot of peo...</td>\n",
       "      <td>\\nit depends on your priorities.  a lot of peo...</td>\n",
       "      <td>\\nit depends on your priorities.  a lot of peo...</td>\n",
       "      <td>\\nit depends on your priorities  a lot of peop...</td>\n",
       "      <td>\\nit depends on your priorities  a lot of peop...</td>\n",
       "      <td>\\nit depends on your priorities  a lot of peop...</td>\n",
       "      <td>depends priorities lot people put higher prior...</td>\n",
       "      <td>depend prioriti lot peopl put higher prioriti ...</td>\n",
       "      <td>depend priority lot people put high priority g...</td>\n",
       "      <td>[depend, priority, lot, people, put, high, pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>excellent automatic found subaru legacy switch...</td>\n",
       "      <td>excel automat found subaru legaci switch sport...</td>\n",
       "      <td>excellent automatic find subaru legacy switch ...</td>\n",
       "      <td>[excellent, automatic, find, subaru, legacy, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>: Ford and his automobile.  I need information...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "      <td>: ford and his automobile.  i need information...</td>\n",
       "      <td>: ford and his automobile.  i need information...</td>\n",
       "      <td>: ford and his automobile.  i need information...</td>\n",
       "      <td>ford and his automobile  i need information o...</td>\n",
       "      <td>ford and his automobile  i need information o...</td>\n",
       "      <td>ford and his automobile  i need information o...</td>\n",
       "      <td>ford automobile need information whether ford ...</td>\n",
       "      <td>ford automobil need inform whether ford partia...</td>\n",
       "      <td>ford automobile need information whether ford ...</td>\n",
       "      <td>[ford, automobile, need, information, whether,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  target  \\\n",
       "0           0  I was wondering if anyone out there could enli...       7   \n",
       "1          17  I recently posted an article asking what kind ...       7   \n",
       "2          29  \\nIt depends on your priorities.  A lot of peo...       7   \n",
       "3          56  an excellent automatic can be found in the sub...       7   \n",
       "4          64  : Ford and his automobile.  I need information...       7   \n",
       "\n",
       "       title                        date  \\\n",
       "0  rec.autos  2022-08-02 13:48:37.251043   \n",
       "1  rec.autos  2022-08-02 13:48:37.251043   \n",
       "2  rec.autos  2022-08-02 13:48:37.251043   \n",
       "3  rec.autos  2022-08-02 13:48:37.251043   \n",
       "4  rec.autos  2022-08-02 13:48:37.251043   \n",
       "\n",
       "                                          lowercased  \\\n",
       "0  i was wondering if anyone out there could enli...   \n",
       "1  i recently posted an article asking what kind ...   \n",
       "2  \\nit depends on your priorities.  a lot of peo...   \n",
       "3  an excellent automatic can be found in the sub...   \n",
       "4  : ford and his automobile.  i need information...   \n",
       "\n",
       "                                     slangs_replaced  \\\n",
       "0  i was wondering if anyone out there could enli...   \n",
       "1  i recently posted an article asking what kind ...   \n",
       "2  \\nit depends on your priorities.  a lot of peo...   \n",
       "3  an excellent automatic can be found in the sub...   \n",
       "4  : ford and his automobile.  i need information...   \n",
       "\n",
       "                               contractions_replaced  \\\n",
       "0  i was wondering if anyone out there could enli...   \n",
       "1  i recently posted an article asking what kind ...   \n",
       "2  \\nit depends on your priorities.  a lot of peo...   \n",
       "3  an excellent automatic can be found in the sub...   \n",
       "4  : ford and his automobile.  i need information...   \n",
       "\n",
       "                                punctuations_removed  \\\n",
       "0  i was wondering if anyone out there could enli...   \n",
       "1  i recently posted an article asking what kind ...   \n",
       "2  \\nit depends on your priorities  a lot of peop...   \n",
       "3  an excellent automatic can be found in the sub...   \n",
       "4   ford and his automobile  i need information o...   \n",
       "\n",
       "                                     numbers_removed  \\\n",
       "0  i was wondering if anyone out there could enli...   \n",
       "1  i recently posted an article asking what kind ...   \n",
       "2  \\nit depends on your priorities  a lot of peop...   \n",
       "3  an excellent automatic can be found in the sub...   \n",
       "4   ford and his automobile  i need information o...   \n",
       "\n",
       "                                       emoji_removed  \\\n",
       "0  i was wondering if anyone out there could enli...   \n",
       "1  i recently posted an article asking what kind ...   \n",
       "2  \\nit depends on your priorities  a lot of peop...   \n",
       "3  an excellent automatic can be found in the sub...   \n",
       "4   ford and his automobile  i need information o...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  wondering anyone could enlighten car saw day 2...   \n",
       "1  recently posted article asking kind rates sing...   \n",
       "2  depends priorities lot people put higher prior...   \n",
       "3  excellent automatic found subaru legacy switch...   \n",
       "4  ford automobile need information whether ford ...   \n",
       "\n",
       "                                       stemmed_words  \\\n",
       "0  wonder anyon could enlighten car saw day 2door...   \n",
       "1  recent post articl ask kind rate singl male dr...   \n",
       "2  depend prioriti lot peopl put higher prioriti ...   \n",
       "3  excel automat found subaru legaci switch sport...   \n",
       "4  ford automobil need inform whether ford partia...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  wonder anyone could enlighten car saw day 2doo...   \n",
       "1  recently post article ask kind rate single mal...   \n",
       "2  depend priority lot people put high priority g...   \n",
       "3  excellent automatic find subaru legacy switch ...   \n",
       "4  ford automobile need information whether ford ...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [wonder, anyone, could, enlighten, car, saw, d...  \n",
       "1  [recently, post, article, ask, kind, rate, sin...  \n",
       "2  [depend, priority, lot, people, put, high, pri...  \n",
       "3  [excellent, automatic, find, subaru, legacy, s...  \n",
       "4  [ford, automobile, need, information, whether,...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "324c2a54-0836-4b4c-809d-47ffcf4f3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-term matrix\n",
    "dictionary = corpora.Dictionary(df[\"tokenized\"])\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in df[\"tokenized\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e0c1c3b-201a-4479-a38d-3c6708a58188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an LDA model on the corpus with 2 topics using Gensim's LdaModel class\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a89b5eb8-c203-48b8-8ef3-64dd54d9c248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic:\n",
      "                                                 Article  Topic\n",
      "0      I was wondering if anyone out there could enli...      2\n",
      "1      I recently posted an article asking what kind ...      2\n",
      "2      \\nIt depends on your priorities.  A lot of peo...      2\n",
      "3      an excellent automatic can be found in the sub...      2\n",
      "4      : Ford and his automobile.  I need information...      2\n",
      "...                                                  ...    ...\n",
      "11309  Secrecy in Clipper Chip\\n\\nThe serial number o...      0\n",
      "11310  Hi !\\n\\nI am interested in the source of FEAL ...      0\n",
      "11311  The actual algorithm is classified, however, t...      0\n",
      "11312  \\n\\tThis appears to be generic calling upon th...      1\n",
      "11313  \\nProbably keep quiet and take it, lest they g...      1\n",
      "\n",
      "[11314 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interpret results\n",
    "documents = df['text'].tolist()\n",
    "article_labels = []\n",
    "for i, doc in enumerate(df[\"tokenized\"]):\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    article_labels.append(dominant_topic)\n",
    "    \n",
    "df_result = pd.DataFrame({\"Article\": documents, \"Topic\": article_labels})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"Table with Articles and Topic:\")\n",
    "print(df_result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03dcfa34-ccec-4217-a409-e38e2552087e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms for Topic #0:\n",
      "['use', 'key', 'system', 'information', 'db', 'encryption', 'chip', 'data', 'available', 'one']\n",
      "\n",
      "Top terms for Topic #1:\n",
      "['would', 'say', 'one', 'think', 'people', 'go', 'know', 'make', 'get', 'see']\n",
      "\n",
      "Top terms for Topic #2:\n",
      "['get', 'use', 'would', 'one', 'like', 'know', 'work', 'problem', 'drive', 'good']\n",
      "\n",
      "Top terms for Topic #3:\n",
      "['government', 'state', 'q', 'armenian', 'people', 'president', 'mr', 'u', 'say', 'right']\n",
      "\n",
      "Top terms for Topic #4:\n",
      "['x', '1', '2', '0', '3', '4', 'file', '5', '6', '7']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print top terms for each topic\n",
    "for topic_id in range(lda_model.num_topics):\n",
    "    print(f\"Top terms for Topic #{topic_id}:\")\n",
    "    top_terms = lda_model.show_topic(topic_id, topn=10)\n",
    "    print([term[0] for term in top_terms])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bdf72e0-4955-49cf-ac0d-8e65c850aba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"use\" (weight: 0.017)\n",
      "- \"key\" (weight: 0.016)\n",
      "- \"system\" (weight: 0.010)\n",
      "- \"information\" (weight: 0.008)\n",
      "- \"db\" (weight: 0.008)\n",
      "- \"encryption\" (weight: 0.007)\n",
      "- \"chip\" (weight: 0.007)\n",
      "- \"data\" (weight: 0.006)\n",
      "- \"available\" (weight: 0.005)\n",
      "- \"one\" (weight: 0.005)\n",
      "\n",
      "Topic 1:\n",
      "- \"would\" (weight: 0.014)\n",
      "- \"say\" (weight: 0.012)\n",
      "- \"one\" (weight: 0.012)\n",
      "- \"think\" (weight: 0.010)\n",
      "- \"people\" (weight: 0.009)\n",
      "- \"go\" (weight: 0.009)\n",
      "- \"know\" (weight: 0.008)\n",
      "- \"make\" (weight: 0.008)\n",
      "- \"get\" (weight: 0.007)\n",
      "- \"see\" (weight: 0.006)\n",
      "\n",
      "Topic 2:\n",
      "- \"get\" (weight: 0.014)\n",
      "- \"use\" (weight: 0.013)\n",
      "- \"would\" (weight: 0.013)\n",
      "- \"one\" (weight: 0.010)\n",
      "- \"like\" (weight: 0.009)\n",
      "- \"know\" (weight: 0.008)\n",
      "- \"work\" (weight: 0.008)\n",
      "- \"problem\" (weight: 0.007)\n",
      "- \"drive\" (weight: 0.006)\n",
      "- \"good\" (weight: 0.006)\n",
      "\n",
      "Topic 3:\n",
      "- \"government\" (weight: 0.009)\n",
      "- \"state\" (weight: 0.008)\n",
      "- \"q\" (weight: 0.008)\n",
      "- \"armenian\" (weight: 0.008)\n",
      "- \"people\" (weight: 0.008)\n",
      "- \"president\" (weight: 0.007)\n",
      "- \"mr\" (weight: 0.006)\n",
      "- \"u\" (weight: 0.005)\n",
      "- \"say\" (weight: 0.005)\n",
      "- \"right\" (weight: 0.005)\n",
      "\n",
      "Topic 4:\n",
      "- \"x\" (weight: 0.041)\n",
      "- \"1\" (weight: 0.038)\n",
      "- \"2\" (weight: 0.025)\n",
      "- \"0\" (weight: 0.022)\n",
      "- \"3\" (weight: 0.015)\n",
      "- \"4\" (weight: 0.013)\n",
      "- \"file\" (weight: 0.012)\n",
      "- \"5\" (weight: 0.010)\n",
      "- \"6\" (weight: 0.008)\n",
      "- \"7\" (weight: 0.008)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top terms for each topic with weight\n",
    "print(\"Top Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7125d085-97ba-4642-8b7e-d8e1dcefec67",
   "metadata": {},
   "source": [
    "**Student Name**: Alasiri Omar Abdullah\n",
    "\n",
    "**Student ID**: IS01082222\n",
    "\n",
    "**Section 03**\n",
    "\n",
    "**Result Interpretation**\n",
    "\n",
    "**Topic 0** seems to be about Cybersecurity / Information Systems theme, we can see the term \"use\" and \"key\" that may indicate that people using information systems, we also see the two terms \"system\" and \"information\" with \"encription\" and \"available\" terms can indicate the wated secyrity for these technologies. The terms clearly align with encryption, databases, and secure information systems. Likely a high coherence score.\n",
    "\n",
    "**Topic 1** seems to be related to General Conversation / Dialogue theme, or people want to know and invistigate a specific event they experienced, we can see the terms \"people\", \"go\", \"know\", \"see\", \"get\" are frequently mentioned, we can also see the terms \"would\" and \"say\" that can indicate that news has spread quickly, meaning that people say things that would happen. Generic verbs and pronouns, possibly too abstract or broad. Likely a low coherence score due to lack of clear thematic connection.\n",
    "\n",
    "**Topic 2** seems to be about General Actions / Common Speech, or getting to work and the problem of driving because of a traffic jam that is frequently occur because many people trying to get to work at the same time causing the traffic jam. We can see the terms \"get\", \"use\", \"work\", \"drive\", and \"problem\" that can indicate that. Similar to Topic 1; includes many functional or frequently used words. May indicate noise or overlapping topics. Low coherence.\n",
    "\n",
    "**Topic 3** looks like about Armenia, which is a small country that share boarders with Turkey and Iran, it seems like a Politics / Geopolitics (possibly Armenian context) theme, Armania is a republic, we can see terms \"armenian\", \"governemnt\", \"state\", \"people\", \"president\", and \"right\", these terms can indicate that the people of Armenia is supporting their government, the terms \"president\", \"mr\", \"say\", and \"right\" can indicate that the armenian people think that their president is saying the right things. Could be strong with more context. Terms suggest a political discussion, but some generic (e.g., \"u\", \"say\", \"right\") terms dilute clarity. Coherence might be moderate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
